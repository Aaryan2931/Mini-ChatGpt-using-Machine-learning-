What is a Confusion Matrix? A confusion matrix is & matrix that summarizes the performance of machine learning model on set of test data_ It is means of displaying the number of accurate and inaccurate instances based on the model's predictions: It is often used to measure the performance of classification models which aim to predict a categorical label for each input instance The matrix displays the number of instances produced by the model on the test data. True positives (TP): occur when the model accurately predicts a positive data point True negatives (TN) occur when the model accurately predicts a negative data point False positives (FP): occur when the model predicts positive data point incorrectly: False negatives (FN): occur when the model mispredicts & negative data point Why do we need a Confusion Matrix? When assessing classification model's performance confusion matrix i5 essential It offers tnorough analysis of true positive_ true negative, false positive, and false negative predictions, facilitating more profound comprehension of model's recall; accuracy, precision_ and overall effectiveness in class distinction When there is an uneven class distribution in a dataset: this matrix is especially helpful in evaluating model's performance beyond basic accuracy metrics

Metrics based on Confusion Matrix Data 1. Accuracy Accuracy is used to measure the performance of the model It is the ratio of Total correct instances to the total instances: Accuracy 4LL TPTTN FPFT For the above case: Accuracy (5+31//5+3+1+1) = 8/10 = 0.8 2. Precision Precision is measure of how accurate models positive predictions are: It is defined as the ratio of true positive predictions to the total number of positive predictions made by the model: Precision TFTT For the above case: Precision 5/(5+1) =5/6 0.8333

Recall Recall measures the effectiveness of classification model in identifying all relevant instances from dataset It is the ratio of the number of true positive (TP) instances to the sum of true positive and false negative (FN) instances Recall For the above case: Recall = 5/(5+1) =5/6 = 0.8333 Note: We use precision when we want to minimize false positives_ crucial in scenarios like spam email detection where misclassifying non-spam message a5 spam is costly: And we use recall when minimizing false negatives is essential, as in medical diagnoses; where identifying all actual positive cases is critical; even if it results in some false positives 4.F1-Score El-score_is used to evaluate the overall performance of classification model It is the harmonic mean of precision and recall #Puciton-Kcll Fl-Score Precidon Recall For the above case: Fl-Score: = (2* 0.8333* 0.83331/( 0.8333+ 0.83331 0.8333 We balance precision and recall with the Fl-score when trade-off between minimizing false positives and false negatives is necessary. such as in information retrieval systems 5. Specificity: Specificity is another important metric in the evaluation of classification models_ particularly in binary classification: It measures the ability of model to correctly identify negative instances Specificity is also known as the True Negative Rate. Specificity Specificity-3/(1+31-3/4-0.75

6. Type 1 and Type 2 error Type 1 error Type 1 error occurs when the model predicts positive instance but it is actually negative_ Precision is affected by false positives as it is the ratio of true positives to the sum of true positives and false positives Type 1 Error example: in a courtroom scenario Type Error, often referred to as false positive_ occurs when the court mistakenly convicts a individual as guilty when; in truth, they are innocent of the alleged crime: This grave error can have profound consequences; leading to the wrongful punishment of an innocent person who did not commit the offense in question: Preventing Type 1 Errors in legal proceedings is paramount to ensuring that justice is accurately served and innocent individuals are protected from unwarranted harm and punishment Type 2 error Type error occurs when the model fails to predict positive instance. Recall is directly affected by false negatives as it is the ratio of true positives to the sum of true positives and false negatives In the context of medical testing Type 2 Error; often known a5 a false negative; occurs wnen diagnostic test fails to detect the presence of a disease in patient who genuinely has it: The consequences of such an error are significant; a5 it may result in delayed diagnosis and subsequent treatment Type 2 Error Precision emphasizes minimizing false positives, while recall focuses on minimizing false negatives For

